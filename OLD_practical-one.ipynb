{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 1\n",
    "\n",
    "### [Carl Henrik Ek](http://carlhenrik.com), University of Cambridge\n",
    "\n",
    "### 2022-10-27"
   ],
   "id": "e3fe854a-5fe5-4fc2-9b79-2ac9aafa5cd8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Decision Making\n",
    "\n",
    "This notebook will form part of your individual submission for the course. The notebook will roughly mimick the parts that are in the PDF worksheet. Your task is to complete the code that is missing in the parts below and answer the questions that we ask. The aim is not for you to solve the worksheet but rather for you to show your understanding of the material in the course, instead of re-running and aiming to get “perfect” results run things, make sure it is correct and then try to explain your results with a few sentences.\n",
    "\n",
    "First we need to implement the surrogate model, we will use a Gaussian process surrogate."
   ],
   "id": "d31cbdb8-6e86-4a6d-823e-2a3a18c40780"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T13:13:20.341569400Z",
     "start_time": "2023-10-17T13:13:10.201806500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial.distance import cdist"
   ],
   "id": "4974ffe4-ee71-4cec-9d53-32b0a496c992"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T13:37:04.181676500Z",
     "start_time": "2023-10-17T13:37:04.173877400Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(X, noise=0.0):\n",
    "    return -(-np.sin(3*X) - X**2 + 0.7*X + noise*np.random.randn(*X.shape))\n",
    "\n",
    "def squared_exponential_computer(x1, x2, theta):\n",
    "    # compute the squared exponential covariance function\n",
    "    # (rbf kernel)\n",
    "    lengthScale = theta[0]\n",
    "    varSigma = theta[1]\n",
    "\n",
    "    d = cdist(x1, x1 if x2 is None else x2)\n",
    "    K = varSigma * np.exp(-np.power(d,2)/lengthScale)\n",
    "    return K\n",
    "\n",
    "def gpposterior(x_star, X, Y, lengthScale, sigma):\n",
    "    # return the posterior estimate of the GP\n",
    "    k_starX = squared_exponential_computer(x_star, X, [lengthScale, sigma])\n",
    "    k_xx = squared_exponential_computer(X, None, [lengthScale, sigma])\n",
    "    k_starstar = squared_exponential_computer(x_star, None, [lengthScale, sigma])\n",
    "\n",
    "    mu = k_starX.dot(np.linalg.inv(k_xx)).dot(Y)\n",
    "    varSigma = k_starstar - (k_starX).dot(np.linalg.inv(k_xx)).dot(k_starX.T)\n",
    "\n",
    "    return mu, varSigma\n",
    "\n",
    "theta = np.zeros((2, ))\n",
    "theta[0] = 0.3  # lengthscale\n",
    "theta[1] = 1.0 # variance"
   ],
   "id": "e11d348f-a369-429c-ab3b-3c6ca2c4799c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the surrogate model up and running we need to implement the acquisition function."
   ],
   "id": "e187b7ac-354f-4d14-ba43-c1dda838f50a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T13:13:39.890939100Z",
     "start_time": "2023-10-17T13:13:39.862549100Z"
    }
   },
   "outputs": [],
   "source": [
    "def expected_improvement(f_star, mu, varSigma):\n",
    "    # return the value of the acquisition function at each\n",
    "    \n",
    "    return alpha"
   ],
   "id": "12861bf8-c509-4993-b9be-726e242e55de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the above code up and running you should be able to reproduce the results that are in Figure 1 in the worksheet. Now lets try to run some additional experiments. First lets evaluate the effect of the initial start locations. Create a plot where on the x-axis have the number of times you have evaluated the true function and on the y-axis have the current minima, run the optimisation loop several times and plot the mean and two standard deviations for the minimal value at each iteration."
   ],
   "id": "7d713b3f-6794-4691-90c9-59e637e56c84"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "implement a loop that tries a set of random-restarts"
   ],
   "id": "36f3d1ef-c3d0-4de2-81b9-da5bf016239a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 1 here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "fe64f9d7-6562-4c90-a640-7f8e0dbea26d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "for j in range(0, n_starts):\n",
    "    for k in range(0, n_evals):\n",
    "        # Your code here"
   ],
   "id": "61758dd3-f3f9-4bc1-92d3-19c471b1c4d9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Explain why the plots looks this way? Does it make a difference how many initial points you start with?}\n",
    "\n",
    "While the function have a local minima so it presents some challenges for optimisation it is still quite easy to find the minima. Let us try make the function a bit more challenging by adding a bit of noise to the function."
   ],
   "id": "5b541b20-7bd7-41b2-99bf-d612c29c0bbc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Implement an additional loop around the previous two loops which alters the amount of noise added."
   ],
   "id": "f26425d6-64d7-4c27-9a82-b35c695f7da7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 2 here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d766bd42-a6a2-4843-9e6a-93db652bf211"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a loop that tries different noise-levels\n",
    "np.random.seed(42)\n",
    "for i in range(0, 10):\n",
    "    y = f(x, noise[i])\n",
    "    for j in range(0, n_starts):\n",
    "        for k in range(0, n_evals):\n",
    "            # Your code here"
   ],
   "id": "6bd8ec49-7d21-4e34-a1be-65fff4247f86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Explain the results by contrasting to the previous none-noisy evaluation. How does the “best” run compare to the “best” run in the previous example?"
   ],
   "id": "1f8b6c59-0978-4dc3-9b03-222b178b00c6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Answer\n",
    "\n",
    "Write your answer to Exercise 3 here\n",
    "\n",
    "As you have probably noticed the kernel-hyperparmeters have a huge effect on the results. This is a desirable effect as this is where we encode our knowledge of the function. We will now do one experiment where we will alter the lengthscale value and see how it effects the results."
   ],
   "id": "2bea9613-60df-47d1-b1e5-079316cefbfb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "For a final extra experiment try to fit the kernel parameters inside the inner-loop. The way to do this is to maximise the marginal likelihood of the surrogate model using gradient descent. You can alter the numpy code that we have implemented to jax instead and which will allow you to use auto-differetiation to compute gradients. Now you can implement a simple gradient descent"
   ],
   "id": "8d9c5841-5b82-42ac-864a-4759cd341b67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as jnp"
   ],
   "id": "d701a34a-54d0-4546-9f86-32872326e96c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_exponential(x1, x2, theta):\n",
    "    # theta[0] - variance\n",
    "    # theta[1] - lengthscale\n",
    "    if x2 == None:\n",
    "        return theta[0]*jnp.exp(-cdist(x1, x1, metric='sqeuclidean')/theta[1]**2)\n",
    "    else:\n",
    "        return theta[0]*jnp.exp(-cdist(x1, x2, metric='sqeuclidean')/theta[1]**2)\n",
    "\n",
    "def logmarginal_likelihood(x, y, theta):\n",
    "    # implement the log-marginal likelihood of a GP\n",
    "    \n",
    "dLdtheta = grad(logmarginal_likelihood, argnums=2)\n",
    "for i in range(1000):\n",
    "    \n",
    "    theta -= dLdtheta(w) * 0.01"
   ],
   "id": "03770795-4d2d-4e69-860d-1a159165c522"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "id": "e711ce99-8615-4b7c-90bc-016fe0c4350b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You can submit the notebook on Moodle. Name your notebook using your CRSid as `crsid_practical-one.ipynb` before submitting to Moodle.\n",
    "\n",
    "The deadline for the submission is Friday the 4th of November at 23:59."
   ],
   "id": "9a0f39e9-4e17-4907-8cd3-b1133bda1837"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
